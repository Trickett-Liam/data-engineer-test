# ğŸ“Œ Apache Beam Batch Job

This repository contains a simple **Apache Beam batch processing pipeline** that:

âœ… Reads transaction data from a CSV file  
âœ… Processes the data  
âœ… Writes the results to a **Gzipped JSONL file**

---

## **âš¡ Prerequisites**
Before setting up, ensure you have the following:

-  **Git** â€“ to clone the repository  
-  **Python 3.10** â€“ required for local execution  
-  **Pyenv** â€“ to ensure the correct Python version  
-  **Docker (and Docker Compose)** â€“ for containerized execution  
-  **Colima** â€“ (Mac/Linux users) required before using Docker  
-  **Google Cloud SDK** â€“ for authentication (if accessing GCS)

---

## **ğŸ³ Setting Up & Running with Docker**
### **1ï¸âƒ£ Clone the Repository**
    git clone https://github.com/Trickett-Liam/data-engineer-test.git
    
    cd data-engineer-test

### **2ï¸âƒ£ Authenticate with Google Cloud**
If the pipeline **reads data from Google Cloud Storage**, log in:
    
    gcloud auth application-default login

Check access to the required GCS bucket:
    
    gsutil ls gs://cloud-samples-data/bigquery/sample-transactions/

If access is **denied**, check your IAM permissions.

### **3ï¸âƒ£ Start Colima (Mac/Linux Users)**
Before using Docker:

    colima start

### **4ï¸âƒ£ Build the Docker Image**
Run:

    make build

### **5ï¸âƒ£ Run the Pipeline with Docker**

    make run

This starts the container and executes `main.py` using the containerâ€™s **virtual environment**.

---

## **ğŸ§ª Running Tests with Docker**
To run tests inside the **Docker container**, execute:

    make test

This uses the containerâ€™s **Apache Beam environment**.

---

## **ğŸ“Œ Notes**
- ğŸš€ The output files in the `output/` directory are **git-ignored**.
- ğŸ— The pipeline uses **DirectRunner**, so **no Cloud Dataflow setup** is required.

---

âœ… **This version ensures Docker users see their setup first, while local setup is still available below!** ğŸš€  
Let me know if you need further refinements! ğŸ”¥
